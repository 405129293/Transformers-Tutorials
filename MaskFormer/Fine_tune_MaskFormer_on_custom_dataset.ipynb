{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e93c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f7eae",
   "metadata": {},
   "source": [
    "## Download toy dataset\n",
    "\n",
    "Here we download a small subset of the ADE20k dataset, which is an important benchmark for semantic segmentation. It contains 150 labels.\n",
    "\n",
    "I've made a small subset just for demonstration purposes (namely the 10 first training and 10 first validation images + segmentation maps). The goal for the model is to overfit this tiny dataset (because that makes sure that it'll work on a larger scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6258473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "def download_data():\n",
    "    url = \"https://www.dropbox.com/s/l1e45oht447053f/ADE20k_toy_dataset.zip?dl=1\"\n",
    "    r = requests.get(url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a23a1",
   "metadata": {},
   "source": [
    "## Define PyTorch dataset and dataloaders\n",
    "\n",
    "Here we define a [custom PyTorch dataset](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). Each item of the dataset consists of an image and a corresponding segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a567c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_tensor, normalize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: Path, train: bool = True, transform = None, num_labels = 150):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        task = 'training' if train else 'validation'\n",
    "        \n",
    "        self.images = list((root_dir / 'images' / task).glob('*.jpg'))\n",
    "        self.images.sort()\n",
    "        self.annotations = list((root_dir / 'annotations' / task).glob('*.png'))\n",
    "        self.annotations.sort()\n",
    "        self.transform = transform\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def reduce_labels(self, mask):\n",
    "         # 0 (ignore) becomes 255. others are shifted by 1\n",
    "        mask = mask - 1 \n",
    "        return mask\n",
    "        \n",
    "    def get_one_hot(self, mask):\n",
    "        mask = mask[mask != 255]\n",
    "        labels_in_mask = np.unique(mask)\n",
    "        one_hot = np.zeros(self.num_labels)\n",
    "        one_hot[labels_in_mask] = 1\n",
    "        return one_hot.astype(int)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.images[idx]))\n",
    "        mask = np.array(Image.open(self.annotations[idx]))\n",
    "        \n",
    "        mask = self.reduce_labels(mask)\n",
    "        labels = self.get_one_hot(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image, mask = transformed['image'], transformed['mask']\n",
    "        \n",
    "        image = to_tensor(image)\n",
    "        labels = labels\n",
    "        \n",
    "        return image, mask, labels\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0008b",
   "metadata": {},
   "source": [
    "We can defined two data augmentations, one for training and one for validating. We used [albumentations](https://albumentations.ai/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fce2dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: albumentations in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: qudida>=0.0.4 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from albumentations) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from albumentations) (0.19.2)\n",
      "Requirement already satisfied, skipping upgrade: opencv-python-headless>=4.1.1 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from albumentations) (4.5.5.62)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from albumentations) (1.22.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.19.1 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: tifffile>=2019.7.26 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2022.2.9)\n",
      "Requirement already satisfied, skipping upgrade: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (9.0.1)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.2 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.4.1 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.16.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=1.1.1 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/home/zuppif/Documents/Work/hugging_face/transformers/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28889f9",
   "metadata": {},
   "source": [
    "MaskFormer needs binary masks as targets, so we need to convert out mask from `[H,W]` to `[C, H, W]`. We can create a custom augmentation for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797e0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import DualTransform\n",
    "\n",
    "class ToBinaryMasks(DualTransform):\n",
    "    def __init__(self, num_labels): \n",
    "        super().__init__(p=1)\n",
    "        self.num_labels = num_labels\n",
    "    \n",
    "    def apply(self, img, **params):\n",
    "        return img\n",
    "    \n",
    "    def apply_to_mask(self, mask, **params):\n",
    "        # we need to convert mask from [W,H] to [C, W, H]\n",
    "        labels = np.arange(self.num_labels)\n",
    "        # broadcast is our friend\n",
    "        binary_masks = mask[np.newaxis,] == labels[:, np.newaxis, np.newaxis]    \n",
    "        return binary_masks.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211dc55",
   "metadata": {},
   "source": [
    "Let's create our data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e403455",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.LongestMaxSize(max_size=1333),\n",
    "    A.RandomCrop(width=640, height=640),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Normalize(),\n",
    "    ToBinaryMasks(num_labels=150)\n",
    "\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    ToBinaryMasks(num_labels=150),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1ee5b",
   "metadata": {},
   "source": [
    "Let's initialize the training + validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe920058",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('./ADE20k_toy_dataset/')\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(root_dir=root_dir, transform=train_transform)\n",
    "valid_dataset = ImageSegmentationDataset(root_dir=root_dir, train=False, transform=valid_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1c9cf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10\n",
      "Number of validation examples: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d69f6",
   "metadata": {},
   "source": [
    "Let's also get the labels from our hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01add08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from huggingface_hub import cached_download, hf_hub_url\n",
    "\n",
    "repo_id = \"datasets/huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename)), \"r\"))\n",
    "id2label = { int(k): v for k, v in id2label.items() }\n",
    "label2id = { v: k for k, v in id2label.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4db49",
   "metadata": {},
   "source": [
    "Let's visualize some of the masks, you can use the following interact cell to select the dataset index and one of the binary masks using their class label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18fbc1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54434462",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67d312f55084e8096d2cadb8d025359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), value=0), Output()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.show_ds(idx)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "vis_dataset = ImageSegmentationDataset(root_dir=root_dir, transform=ToBinaryMasks(150))\n",
    "\n",
    "def show_ds(idx):\n",
    "    image, binary_masks, labels = vis_dataset[idx]\n",
    "    image = image.permute(1,2,0)\n",
    "    labels_in_masks = [v for k,v in id2label.items() if labels[k] != 0]\n",
    "    \n",
    "    labels_dropdown = widgets.Dropdown(options=labels_in_masks)\n",
    "   \n",
    "    \n",
    "    def show_mask(*args):\n",
    "        ax2.imshow(binary_masks[label2id[labels_dropdown.value]])\n",
    "   \n",
    "    fig, (ax1, ax2)  = plt.subplots(ncols=2)\n",
    "    ax1.imshow(image)\n",
    "    ax2.imshow(binary_masks[label2id[labels_dropdown.value]])\n",
    "    \n",
    "    labels_dropdown.observe(show_mask, 'value')\n",
    "\n",
    "    return labels_dropdown\n",
    "   \n",
    "\n",
    "\n",
    "interact(show_ds, idx=range(len(vis_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b52940",
   "metadata": {},
   "source": [
    "Let's define our two dataloader with a custom `collate_fn`. We are using `MaskFormerFeatureExtractor.encode_inputs` the encode our inputs, under the hook they will be pad to the biggest image in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a528c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MaskFormerFeatureExtractor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "feature_extractor = MaskFormerFeatureExtractor()\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    pixel_values = inputs[0]\n",
    "    annotations = [\n",
    "        {\"masks\": masks, \"labels\": labels} for masks, labels in zip(inputs[1], inputs[2])\n",
    "    ]\n",
    "    return feature_extractor.encode_inputs(pixel_values, annotations=annotations, return_tensors='pt')\n",
    "\n",
    "def valid_collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    pixel_values = inputs[0]\n",
    "    annotations = [\n",
    "        {\"masks\": masks, \"labels\": labels} for masks, labels in zip(inputs[1], inputs[2])\n",
    "    ]\n",
    "    return  feature_extractor(list(pixel_values), annotations=annotations, return_tensors='pt')\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=2, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, collate_fn=valid_collate_fn, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee2757c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 640, 640]),\n",
       " torch.Size([2, 150, 640, 640]),\n",
       " torch.Size([2, 150]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "batch[\"pixel_values\"].shape, batch[\"mask_labels\"].shape, batch[\"class_labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b9d437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 800, 1120]),\n",
       " torch.Size([2, 150, 800, 1120]),\n",
       " torch.Size([2, 150]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(valid_dataloader))\n",
    "\n",
    "batch[\"pixel_values\"].shape, batch[\"mask_labels\"].shape, batch[\"class_labels\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02dfa8f",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc19e2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Here we load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e6eb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuppif/Documents/Work/hugging_face/transformers/.venv/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MaskFormerForInstanceSegmentation\n",
    "\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-small-ade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad89fb",
   "metadata": {},
   "source": [
    "We can freeze the encoder to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db5087ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.model.pixel_level_module.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1a640",
   "metadata": {},
   "source": [
    "train + validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e977da30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924cf14e1e4a47249ccf63766c6c843a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(4)):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    batch_bar = tqdm(train_dataloader, leave=False)\n",
    "    for batch in batch_bar:\n",
    "        # put the input in the right device\n",
    "        batch = { k: v.to(device) for k, v in batch.items() }\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward \n",
    "        outputs = model(**batch)\n",
    "        # backward \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        batch_bar.set_postfix({'loss': loss.item()})\n",
    "        batch_bar.update()\n",
    "    batch_bar.reset()\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(valid_dataloader, leave=False)\n",
    "    for batch in batch_bar:\n",
    "        # forward \n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"pixel_values\"].to(device))\n",
    "            # use the feature extractor to get the final segmentation\n",
    "            mask_labels = batch[\"mask_labels\"].cpu()\n",
    "            # use the feature extractor to postprocess the outputs\n",
    "            semantic_segmentation = feature_extractor.post_process_semantic_segmentation(outputs, target_size=mask_labels.shape[2:]).cpu()\n",
    "            target_segmentation = mask_labels.argmax(dim=1)\n",
    "            accuracy = (semantic_segmentation == target_segmentation).sum() / target_segmentation.numel()\n",
    "            batch_bar.set_postfix({'accuracy': accuracy.item()})\n",
    "    batch_bar.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf8dfd",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Let's see how we can easily visualize the results from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb45ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask_labels, labels = valid_dataset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = feature_extractor(image)\n",
    "    outputs = model(**inputs)\n",
    "    semantic_segmentation = feature_extractor.post_process_semantic_segmentation(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd08ed0",
   "metadata": {},
   "source": [
    "Let's see the segmentation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cba06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(semantic_segmentation[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
